{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f422671-6cce-4134-b653-24d2c7d28563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address        Load       Tokens  Owns (effective)  Host ID                               Rack \n",
      "UN  192.168.192.4  242.4 KiB  16      70.5%             770d84d1-39e4-478f-991a-62619ebdb874  rack1\n",
      "UN  192.168.192.3  25.55 KiB  16      65.0%             9611be36-0777-4331-954c-109cd67c8ccb  rack1\n",
      "UN  192.168.192.2  25.55 KiB  16      64.5%             bb63e485-ad0b-461a-bdff-336281899fd8  rack1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nodetool status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f218e7a5-8ab3-4281-a898-a7107b0bc005",
   "metadata": {},
   "source": [
    "# Part 1: Station Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25d5395c-744e-45b3-8126-68b75a835b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cassandra.cluster:Host 192.168.192.2:9042 has been marked down\n",
      "WARNING:cassandra.pool:Error attempting to reconnect to 192.168.192.2:9042, scheduling retry in 2.0 seconds: [Errno None] Tried connecting to [('192.168.192.2', 9042)]. Last error: timed out\n",
      "WARNING:cassandra.pool:Error attempting to reconnect to 192.168.192.2:9042, scheduling retry in 4.48 seconds: [Errno None] Tried connecting to [('192.168.192.2', 9042)]. Last error: timed out\n",
      "WARNING:cassandra.pool:Error attempting to reconnect to 192.168.192.2:9042, scheduling retry in 9.2 seconds: [Errno None] Tried connecting to [('192.168.192.2', 9042)]. Last error: timed out\n",
      "WARNING:cassandra.pool:Error attempting to reconnect to 192.168.192.2:9042, scheduling retry in 17.12 seconds: [Errno None] Tried connecting to [('192.168.192.2', 9042)]. Last error: timed out\n",
      "WARNING:cassandra.pool:Error attempting to reconnect to 192.168.192.2:9042, scheduling retry in 36.16 seconds: [Errno 113] Tried connecting to [('192.168.192.2', 9042)]. Last error: No route to host\n",
      "WARNING:cassandra.pool:Error attempting to reconnect to 192.168.192.2:9042, scheduling retry in 71.04 seconds: [Errno 113] Tried connecting to [('192.168.192.2', 9042)]. Last error: No route to host\n",
      "WARNING:cassandra.pool:Error attempting to reconnect to 192.168.192.2:9042, scheduling retry in 145.92 seconds: [Errno 113] Tried connecting to [('192.168.192.2', 9042)]. Last error: No route to host\n",
      "WARNING:cassandra.pool:Error attempting to reconnect to 192.168.192.2:9042, scheduling retry in 286.72 seconds: [Errno 113] Tried connecting to [('192.168.192.2', 9042)]. Last error: No route to host\n"
     ]
    }
   ],
   "source": [
    "# Connect to the Cassandra cluster using this code:\n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['p6-db-1', 'p6-db-2', 'p6-db-3'])\n",
    "cass = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a81733b-d431-4cb7-9448-e0859401eb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x77e198aed3f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then write code to do the following:\n",
    "\n",
    "# drop a weather keyspace if it already exists\n",
    "cass.execute(\"DROP KEYSPACE IF EXISTS weather;\")\n",
    "\n",
    "# create a weather keyspace with 3x replication\n",
    "cass.execute(\"\"\"\n",
    "CREATE KEYSPACE weather WITH\n",
    "replication = {'class': 'SimpleStrategy', 'replication_factor': 3};\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93b753b3-5d58-4192-aa45-5a03b73613de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x77e198ab2770>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use the weather keyspace\n",
    "cass.execute(\"use weather\")\n",
    "\n",
    "# inside weather, create a station_record type containing two ints: tmin and tmax\n",
    "cass.execute(\"\"\"\n",
    "create type station_record(\n",
    "    tmin INT,\n",
    "    tmax INT\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b836e271-51bd-4bc4-a00d-23ca67d7fe1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x77e198aed2d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inside weather, create a station table\n",
    "#The stations table should have four columns: \n",
    "#id (text), name (text), date (date), record (weather.station_record):\n",
    "\n",
    "# id is a partition key and corresponds to a station's ID (like 'USC00470273')\n",
    "# date is a cluster key, ascending\n",
    "# name is a static field (because there is only one name per ID). Example: 'UW ARBORETUM - MADISON'\n",
    "# record is a regular field because there will be many records per station partition.\n",
    "\n",
    "cass.execute(\"\"\"\n",
    "create table stations(\n",
    "    id TEXT,\n",
    "    name TEXT static,\n",
    "    date DATE,\n",
    "    record weather.station_record,\n",
    "    PRIMARY KEY ((id), date)\n",
    ") WITH CLUSTERING ORDER BY (date ASC)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34428314-3135-4530-b571-fbdc29c58b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"CREATE TABLE weather.stations (\\n    id text,\\n    date date,\\n    name text static,\\n    record station_record,\\n    PRIMARY KEY (id, date)\\n) WITH CLUSTERING ORDER BY (date ASC)\\n    AND additional_write_policy = '99p'\\n    AND bloom_filter_fp_chance = 0.01\\n    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\\n    AND cdc = false\\n    AND comment = ''\\n    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}\\n    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\\n    AND memtable = 'default'\\n    AND crc_check_chance = 1.0\\n    AND default_time_to_live = 0\\n    AND extensions = {}\\n    AND gc_grace_seconds = 864000\\n    AND max_index_interval = 2048\\n    AND memtable_flush_period_in_ms = 0\\n    AND min_index_interval = 128\\n    AND read_repair = 'BLOCKING'\\n    AND speculative_retry = '99p';\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q1\n",
    "# What is the Schema of stations?\n",
    "cass.execute(\"describe table weather.stations\").one().create_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ba18cd7-4d56-4623-b811-c40c2abaa207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-25114845-255c-4ce6-a168-162fc84e14b3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.4.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.12/3.4.0/spark-cassandra-connector_2.12-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector_2.12;3.4.0!spark-cassandra-connector_2.12.jar (136ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector-driver_2.12/3.4.0/spark-cassandra-connector-driver_2.12-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0!spark-cassandra-connector-driver_2.12.jar (73ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-core-shaded/4.13.0/java-driver-core-shaded-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-core-shaded;4.13.0!java-driver-core-shaded.jar (282ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-mapper-runtime/4.13.0/java-driver-mapper-runtime-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-mapper-runtime;4.13.0!java-driver-mapper-runtime.jar(bundle) (35ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.10/commons-lang3-3.10.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-lang3;3.10!commons-lang3.jar (49ms)\n",
      "downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar ...\n",
      "\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.8!paranamer.jar(bundle) (32ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.11/scala-reflect-2.12.11.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.11!scala-reflect.jar (139ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/native-protocol/1.5.0/native-protocol-1.5.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#native-protocol;1.5.0!native-protocol.jar(bundle) (39ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-shaded-guava/25.1-jre-graal-sub-1/java-driver-shaded-guava-25.1-jre-graal-sub-1.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1!java-driver-shaded-guava.jar (166ms)\n",
      "downloading https://repo1.maven.org/maven2/com/typesafe/config/1.4.1/config-1.4.1.jar ...\n",
      "\t[SUCCESSFUL ] com.typesafe#config;1.4.1!config.jar(bundle) (33ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.26/slf4j-api-1.7.26.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.26!slf4j-api.jar (31ms)\n",
      "downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/4.1.18/metrics-core-4.1.18.jar ...\n",
      "\t[SUCCESSFUL ] io.dropwizard.metrics#metrics-core;4.1.18!metrics-core.jar(bundle) (34ms)\n",
      "downloading https://repo1.maven.org/maven2/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar ...\n",
      "\t[SUCCESSFUL ] org.hdrhistogram#HdrHistogram;2.1.12!HdrHistogram.jar(bundle) (35ms)\n",
      "downloading https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.3!reactive-streams.jar (30ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar ...\n",
      "\t[SUCCESSFUL ] com.github.stephenc.jcip#jcip-annotations;1.0-1!jcip-annotations.jar (29ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/spotbugs/spotbugs-annotations/3.1.12/spotbugs-annotations-3.1.12.jar ...\n",
      "\t[SUCCESSFUL ] com.github.spotbugs#spotbugs-annotations;3.1.12!spotbugs-annotations.jar (35ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (28ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-query-builder/4.13.0/java-driver-query-builder-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-query-builder;4.13.0!java-driver-query-builder.jar(bundle) (35ms)\n",
      ":: resolution report :: resolve 5546ms :: artifacts dl 1272ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.4.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   18  |   18  |   0   ||   18  |   18  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-25114845-255c-4ce6-a168-162fc84e14b3\n",
      "\tconfs: [default]\n",
      "\t18 artifacts copied, 0 already retrieved (18067kB/68ms)\n",
      "24/04/14 17:40:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1313"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import substring, col\n",
    "import pandas as pd\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"p6\")\n",
    "         .config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.4.0')\n",
    "         .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\n",
    "         .getOrCreate())\n",
    "\n",
    "station_data = spark.read.text(\"/nb/ghcnd-stations.txt\")\n",
    "\n",
    "#substring(str, pos, len)\n",
    "#(end - start )+ 1\n",
    "station_data = station_data.select(\n",
    "    substring('value', 1, 11).alias('ID'),  \n",
    "    substring('value', 39, 2).alias('STATE'),\n",
    "    substring('value', 42, 30).alias('NAME')\n",
    ")\n",
    "\n",
    "# printing Dataframe schema to get the column names\n",
    "#station_data.printSchema()\n",
    "\n",
    "# visualizing the dataframe\n",
    "#station_data.show(truncate=False)\n",
    "\n",
    "wi_station_df = station_data.filter(col('STATE') == 'WI')\n",
    "wi_station = wi_station_df.collect() \n",
    "\n",
    "for row in wi_station:\n",
    "    wi_insert = cass.prepare(\n",
    "            f\"INSERT INTO weather.stations (id, name) VALUES (?, ?)\"\n",
    "        )\n",
    "    cass.execute(wi_insert, (row['ID'], row['NAME']))\n",
    "    \n",
    "output = pd.DataFrame(cass.execute(\"SELECT COUNT(*) FROM weather.stations\"))\n",
    "output.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a05c1e0d-adfc-4d52-8651-62892f3b712b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AMBERG 1.3 SW                 '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2\n",
    "#what is the name corresponding to station ID US1WIMR0003?\n",
    "output = pd.DataFrame(cass.execute(\"\"\"\n",
    "SELECT name\n",
    "FROM weather.stations\n",
    "WHERE id = 'US1WIMR0003'\n",
    "\"\"\"))\n",
    "output.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f03c9858-7902-4d4c-b216-4258747bc265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9014250178872933741"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3\n",
    "# what is the token for the USC00470273 station?\n",
    "output = pd.DataFrame(cass.execute(\"\"\"\n",
    "SELECT TOKEN(ID)\n",
    "FROM weather.stations\n",
    "WHERE id = 'USC00470273'\n",
    "\"\"\"))\n",
    "\n",
    "output.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6ccb980-12a1-4758-9a05-18f3a1ed72bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_184/2112290626.py:24: DeprecationWarning: ResultSet indexing support will be removed in 4.0. Consider using ResultSet.one() to get a single row.\n",
      "  target_token = result[0][0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-8436275424918165244"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q4\n",
    "\n",
    "# what is the first vnode token in the ring following the token for USC00470273?\n",
    "import re #To read the output using regex\n",
    "from subprocess import check_output\n",
    "\n",
    " \n",
    "\n",
    "result = cass.execute(\"\"\"\n",
    "SELECT TOKEN(ID)\n",
    "FROM weather.stations\n",
    "WHERE id = 'USC00470273'\n",
    "\"\"\")\n",
    "#Use check_output to run nodetool ring\n",
    "output = check_output([\"nodetool\", \"ring\"])\n",
    "tokens = re.findall(r\"-?\\d{19}\", output.decode(\"utf-8\")) #Use regex to find all the token numbers in the output\n",
    "\n",
    "#Each token in tokens is a string so covert to ints with list comphrehension\n",
    "tokens = [int(token) for token in tokens]\n",
    "\n",
    "#Find the next token after the target token\n",
    "#sort the tokens\n",
    "tokens.sort()\n",
    "target_token = result[0][0]\n",
    "next_token = None\n",
    "#Loop through the tokens until we find the next bigger token than the current token for USC00470273\n",
    "for token in tokens:\n",
    "    if token > target_token:\n",
    "        next_token = token\n",
    "        break #Break because we want to find just the next token thats bigger\n",
    "next_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaec6cb-b007-4771-8573-0cfe7c209896",
   "metadata": {},
   "source": [
    "# Part 2 Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48b28fdf-fbc1-4214-9f1a-3b6fa24df94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  records.zip\n",
      "  inflating: records.parquet/part-00000-574ab704-2431-4c8b-9d88-6c635a467b99-c000.snappy.parquet  \n",
      " extracting: records.parquet/._SUCCESS.crc  \n",
      "  inflating: records.parquet/part-00002-574ab704-2431-4c8b-9d88-6c635a467b99-c000.snappy.parquet  \n",
      "  inflating: records.parquet/part-00001-574ab704-2431-4c8b-9d88-6c635a467b99-c000.snappy.parquet  \n",
      "  inflating: records.parquet/part-00003-574ab704-2431-4c8b-9d88-6c635a467b99-c000.snappy.parquet  \n",
      " extracting: records.parquet/.part-00003-574ab704-2431-4c8b-9d88-6c635a467b99-c000.snappy.parquet.crc  \n",
      " extracting: records.parquet/_SUCCESS  \n",
      " extracting: records.parquet/.part-00000-574ab704-2431-4c8b-9d88-6c635a467b99-c000.snappy.parquet.crc  \n",
      " extracting: records.parquet/.part-00001-574ab704-2431-4c8b-9d88-6c635a467b99-c000.snappy.parquet.crc  \n",
      " extracting: records.parquet/.part-00002-574ab704-2431-4c8b-9d88-6c635a467b99-c000.snappy.parquet.crc  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Unzip records.zip cell\n",
    "! unzip -o records.zip\n",
    "records_df = spark.read.parquet(\"records.parquet\")\n",
    "\n",
    "#Import statements to answer q5\n",
    "import grpc\n",
    "import station_pb2\n",
    "import station_pb2_grpc\n",
    "import datetime #Using date time to convert the row.date to %Y-%m-%d format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d657633-8346-49c2-abf1-8006cf05e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_output = records_df.groupBy(\"station\", \"date\").pivot(\"element\", [\"TMIN\", \"TMAX\"]).sum(\"value\")\n",
    "\n",
    "channel = grpc.insecure_channel(f\"localhost:5440\")\n",
    "stub = station_pb2_grpc.StationStub(channel)\n",
    "    \n",
    "for row in record_output.collect():\n",
    "    date_output = datetime.datetime.strptime(row.date, '%Y%m%d') #Convert to YYYY-MM-DD TIME\n",
    "    date_output_formatted = date_output.strftime(\"%Y-%m-%d\") #Remove the TIME\n",
    "    response = stub.RecordTemps(station_pb2.RecordTempsRequest(station = row.station, date = date_output_formatted, tmin = int(row.TMIN), tmax = int(row.TMAX)))\n",
    "    # error = response.error\n",
    "    # print(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "630b2323-d2df-412e-9d07-00861b68425e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q5\n",
    "#what is the max temperature ever seen for station USW00014837?\n",
    "#Request for station \"USW00014837\n",
    "request = station_pb2.StationMaxRequest(station = \"USW00014837\")\n",
    "\n",
    "max_response= stub.StationMax(request)\n",
    "max_response.tmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c814a8-eb4e-4a48-853d-72d814f9541b",
   "metadata": {},
   "source": [
    "# Part 3: Spark Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bee1896f-b8f6-4ea2-9be0-8365773535c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view in Spark named stations that corresponds to the stations table in Cassandra.\n",
    "# Hint: you already enabled CassandraSparkExtensions when creating your Spark session, \n",
    "#so you can create a Spark DataFrame corresponding to a Cassandra table like this:\n",
    "stations_df_spark = (spark.read.format(\"org.apache.spark.sql.cassandra\")\n",
    ".option(\"spark.cassandra.connection.host\", \"p6-db-1,p6-db-2,p6-db-3\")\n",
    ".option(\"keyspace\", 'weather')\n",
    ".option(\"table\", 'stations')\n",
    ".load())\n",
    "\n",
    "stations_df_spark.createOrReplaceTempView(\"stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bcfb582-c49d-42d6-b579-024a089a7b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='stations', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q6\n",
    "#what tables/views are available in the Spark catalog?\n",
    "tables_list = spark.catalog.listTables()\n",
    "#len(tables_list) #Only one table\n",
    "tables_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a38d6fd2-d2fc-441c-9ff4-8d1cba611e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'USR0000WDDG': 102.06849315068493,\n",
       " 'USW00014837': 105.62739726027397,\n",
       " 'USW00014839': 89.6986301369863,\n",
       " 'USW00014898': 102.93698630136986}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cassandra.connection:Heartbeat failed for connection (131810812938128) to 192.168.192.2:9042\n",
      "24/04/14 17:44:10 WARN ChannelPool: [s0|p6-db-2/192.168.192.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=3f10f974-2036-43e8-868e-ba60c4cab8f1, APPLICATION_NAME=Spark-Cassandra-Connector-local-1713116453606}): failed to send request (java.nio.channels.NotYetConnectedException))\n",
      "24/04/14 17:44:17 WARN ChannelPool: [s0|p6-db-2/192.168.192.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=3f10f974-2036-43e8-868e-ba60c4cab8f1, APPLICATION_NAME=Spark-Cassandra-Connector-local-1713116453606}): failed to send request (java.nio.channels.NotYetConnectedException))\n"
     ]
    }
   ],
   "source": [
    "#q7\n",
    "#what is the average difference between tmax and tmin, \n",
    "#for each of the four stations that have temperature records?\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "output_df = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM stations\n",
    "WHERE record IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "#Separate the record column row objects into their own columns\n",
    "output_df = (output_df.withColumn('temp_min', col('record')['tmin'])\n",
    "            .withColumn('temp_max', col('record')['tmax'])\n",
    "            .withColumn('temp_diff', col('temp_max') - col('temp_min')))\n",
    "\n",
    "output_df = output_df.drop(\"temp_min\", \"temp_max\") # Drop the min and max columns\n",
    "\n",
    "#Convert to pandas, group by id, calculate the mean of temp_diff, convert to dict,\n",
    "average_temp_diff_dict = output_df.toPandas().groupby(\"id\").mean(\"temp_diff\").to_dict()['temp_diff']\n",
    "average_temp_diff_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e92d12-60d6-45c4-9bda-1b5b7b871c75",
   "metadata": {},
   "source": [
    "# Part 4: Disaster Strikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f75eb786-db16-4fe9-a365-55613d624dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Docker command 'docker stop p6-db-2' to kill the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad8d5336-1403-489e-98a8-31062f9ebcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/14 17:44:26 WARN ChannelPool: [s0|p6-db-2/192.168.192.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=3f10f974-2036-43e8-868e-ba60c4cab8f1, APPLICATION_NAME=Spark-Cassandra-Connector-local-1713116453606}): failed to send request (java.nio.channels.NotYetConnectedException))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address        Load       Tokens  Owns (effective)  Host ID                               Rack \n",
      "UN  192.168.192.4  98.03 KiB  16      100.0%            770d84d1-39e4-478f-991a-62619ebdb874  rack1\n",
      "UN  192.168.192.3  87.73 KiB  16      100.0%            9611be36-0777-4331-954c-109cd67c8ccb  rack1\n",
      "DN  192.168.192.2  87.74 KiB  16      100.0%            bb63e485-ad0b-461a-bdff-336281899fd8  rack1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#q8\n",
    "#what does nodetool status output?\n",
    "\n",
    "#Use the ! COMMAND to show the output in a cell\n",
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f9f9476-1071-498b-ba3d-4c5cf5e9cc99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'need 3 replicas, but only have 2'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/14 17:52:14 WARN ChannelPool: [s0|p6-db-2/192.168.192.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=3f10f974-2036-43e8-868e-ba60c4cab8f1, APPLICATION_NAME=Spark-Cassandra-Connector-local-1713116453606}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n"
     ]
    }
   ],
   "source": [
    "#q9\n",
    "#if you make a StationMax RPC call, what does the error field contain in StationMaxReply reply?\n",
    "request = station_pb2.StationMaxRequest(station = \"USW00014837\")\n",
    "\n",
    "max_response = stub.StationMax(request)\n",
    "max_response.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f49b2e3c-123b-4f7f-84ec-bfef5b7e6cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/14 17:48:16 WARN ChannelPool: [s0|p6-db-2/192.168.192.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=3f10f974-2036-43e8-868e-ba60c4cab8f1, APPLICATION_NAME=Spark-Cassandra-Connector-local-1713116453606}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "24/04/14 17:49:19 WARN ChannelPool: [s0|p6-db-2/192.168.192.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=3f10f974-2036-43e8-868e-ba60c4cab8f1, APPLICATION_NAME=Spark-Cassandra-Connector-local-1713116453606}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "24/04/14 17:50:15 WARN ChannelPool: [s0|p6-db-2/192.168.192.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=3f10f974-2036-43e8-868e-ba60c4cab8f1, APPLICATION_NAME=Spark-Cassandra-Connector-local-1713116453606}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "24/04/14 17:51:12 WARN ChannelPool: [s0|p6-db-2/192.168.192.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=3f10f974-2036-43e8-868e-ba60c4cab8f1, APPLICATION_NAME=Spark-Cassandra-Connector-local-1713116453606}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n"
     ]
    }
   ],
   "source": [
    "#q10\n",
    "# if you make a RecordTempsRequest RPC call, what does error contain in the RecordTempsReply reply?\n",
    "\n",
    "#make up data\n",
    "request = stub.RecordTemps(station_pb2.RecordTempsRequest(station = \"UUUU1111111\", date = \"2022-01-01\", tmin = -1000, tmax = 1000))\n",
    "request.error # should be ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e199d8d-4ab5-4db8-a4a1-567577bfeefd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
